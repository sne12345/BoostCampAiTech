## 선형 SVM 분류

### ㅇ과 ㅁ의 두 범주를 나누는 분류 문제를 푼다고 가정해보자.
![image](https://user-images.githubusercontent.com/51853700/135063562-5f4e9272-49b1-4e9a-b83a-459fffe5926c.png)

* 구분을 위해 두 범주 사이에 선을 긋는다고 하면, 위 그림과 같이 다양한 방법이 존재한다.

* 하지만 만약 새로운 데이터가 들어와서 선 부근쪽에 위치하면, 그 데이터는 o인지 ㅁ인지 판단하기 애매해진다.

![image](https://user-images.githubusercontent.com/51853700/135063724-6bae0687-a222-4d1f-98d8-bed6e406c1df.png)

* 위 그림처럼, 새로운 데이터가 들어왔을 때, 해당 데이터를 구분시켜 줄 기준이 되는 선을 **서포트 벡터**라고 한다.

* 두 개의 그룹을 구분하는 일정한 거리는 **마진(Margin)** 이라고 하며,

* 마진이 최대값을 가질 때의 중간 경계선을 최적 초평면(optimal hyperplane) 또는 결정 경계(Decision Boundary)라고 한다.

* 따라서 SVM은 각 그룹이 최대로 떨어질 수 있는 거리(최대마진)를 찾고

* **해당 거리의 중간지점(결정 경계)으로 그룹을 구분하는 기법이다.**




## 비선형 SVM 분류

* 선형 SVM 분류기는 효율적이고 많은 경우에 아주 잘 작동하지만, 데이터 셋이 비선형일 땐 선형적으로 구분할 수 없다.

* 비선형 데이터를 선형적으로 다루기 위해서는 고차원 공간으로 데이터를 변환해 사용한다.




## 서포트 벡터 회귀(SVR,Support Vector Regression)

![image](https://user-images.githubusercontent.com/51853700/135064128-fc9f4b26-23ef-4161-82d6-30d3d32a0491.png)

* 회귀의 목표는 분류의 목표와 반대이다.

* 결정경계의 마진을 최대한 크게 하고, 마진 안으로 회귀 데이터가 최대한 많이 들어가도록 학습하는 것이다.

* 마진 폭은 epilson 이라는 하이퍼파라미터를 사용하여 조절합니다.



## SVM의 장단점

#### 장점

* 비선형 분리 데이터를 커널트릭을 사용하여 분류 모델링 가능
* 고차원 공간에서 원할하게 작동함
* 텍스트 분류 및 이미지 분류에 효과적임

#### 단점

* 데이터가 너무 많으면 속도가 느리고 메모리적으로 힘듦
* 확률 추정치를 제공하지 않음.
* 선형 커널은 선형의 분리 가능한 데이터인 경우 로지스틱 회귀분석과 거의 유사함.



출처 : https://wooono.tistory.com/111
